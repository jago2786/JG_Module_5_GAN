{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n#from tensorflow import keras\nfrom tensorflow.keras import layers, ops\n#from tensorflow_docs.vis import embed\n#import tensorflow_addons as tfa\nfrom itertools import zip_longest\nimport shutil\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.utils import make_grid\nimport PIL\n\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nautotune = tf.data.AUTOTUNE\nimport os\n#os.environ[\"KERAS_BACKEND\"]= \"tensorflow\"\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T22:16:38.048170Z","iopub.execute_input":"2025-03-04T22:16:38.048646Z","iopub.status.idle":"2025-03-04T22:16:38.222387Z","shell.execute_reply.started":"2025-03-04T22:16:38.048614Z","shell.execute_reply":"2025-03-04T22:16:38.220361Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"#Load datasets into monet and photo, gotten from https://www.kaggle.com/code/sai10py/gans-art-creation\ninput_img_size = (256,256,3)\n\nDataset_Path = '/kaggle/input/gan-getting-started'\nkernel_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\ngamma_init=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.02)\nbuffersize=256\nbatch_size=1\n\nMonet_names = os.listdir(Dataset_Path + '/monet_jpg/')\nMonet_paths = tf.io.gfile.glob(str(Dataset_Path + '/monet_jpg/' + '*.jpg'))\n#print(tf.io.glob.glob(Dataset_Path + '/monet_frec/*.tfrec'))\n#for i in tf.io.gfile(Dataset_Path + '/monet_frec/*.tfrec'):\n#    Monet_names.append(Dataset_Path + i)\nPhoto_names = os.listdir(Dataset_Path + '/photo_jpg/')\nPhoto_paths = tf.io.gfile.glob(str(Dataset_Path + '/photo_jpg/' + '*.jpg'))\n#Monet_names = tf.io.gfile.glob(str(Dataset_Path + '/monet_frec/*.tfrec'))\nprint(len(Monet_names))\nprint(Monet_names[0])\nprint(len(Photo_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:06.543880Z","iopub.execute_input":"2025-03-04T19:41:06.544449Z","iopub.status.idle":"2025-03-04T19:41:07.534592Z","shell.execute_reply.started":"2025-03-04T19:41:06.544400Z","shell.execute_reply":"2025-03-04T19:41:07.533270Z"}},"outputs":[{"name":"stdout","text":"300\nf4413e97bd.jpg\n7038\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"This model is meant to use a generative adversarial networks to build a discriminator between real Monet paintings and photos manipulated to look like Monet paintings, and a generator that is meant to turn photos into Monet-style images. There are 300 Monet paintings and 7038 photographs. All images are 256x256x3, and therefore there is no need to clean the datasets.","metadata":{}},{"cell_type":"code","source":"#Gotten from https://www.kaggle.com/code/sai10py/gans-art-creation\ndef decode_img(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = (tf.cast(img, tf.float32)/127.5)-1\n    img = tf.reshape(img, [256, 256, 3])\n    return img\n\nMonet_Tensor = tf.data.Dataset.from_tensor_slices(Monet_paths)\nPhoto_Tensor = tf.data.Dataset.from_tensor_slices(Photo_paths)\n\nMonet_data = Monet_Tensor.map(decode_img, num_parallel_calls=tf.data.AUTOTUNE)\nPhoto_data = Photo_Tensor.map(decode_img, num_parallel_calls = tf.data.AUTOTUNE)\n#Monet_dataset = Monet_Tensor.map(mpimg.imread(Monet_names))\n#Photo_dataset = Photo_Tensor.map(mpimg.imread(Photo_names))\n#print(tf.shape(Monet_Tensor))\n\nmonet_data = Monet_data.shuffle(len(Monet_paths)).batch(1).prefetch(tf.data.AUTOTUNE)\nphoto_data = Photo_data.shuffle(len(Photo_paths)).batch(1).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:07.537015Z","iopub.execute_input":"2025-03-04T19:41:07.537393Z","iopub.status.idle":"2025-03-04T19:41:07.601100Z","shell.execute_reply.started":"2025-03-04T19:41:07.537348Z","shell.execute_reply":"2025-03-04T19:41:07.600002Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#Monet_Train_images = tf.data.Dataset.from_tensor_slices()\n\n#Monet_train_images = []\n#for i in Monet_names_train:\n    #temp_image = torchvision.transforms.ToTensor()\n#    Monet_train_images.append(tf.constant(mpimg.imread(Dataset_Path + '/monet_jpg/' + str(i)),shape=(1,256,256,3)))\n\n#Monet_test_images = []\n#for i in Monet_names_test:\n#    Monet_test_images.append(tf.constant(mpimg.imread(Dataset_Path + '/monet_jpg/' + str(i)),shape=(1,256,256,3)))\n\n#Photo_train_images = []\n#for i in Photo_names_train:\n#    Photo_train_images.append(tf.constant(mpimg.imread(Dataset_Path + '/photo_jpg/' + str(i)),shape=(1,256,256,3)))\n\n#Photo_test_images = []\n#for i in Photo_names_test:\n#    Photo_test_images.append(tf.constant(mpimg.imread(Dataset_Path + '/photo_jpg/' + str(i)),shape=(1,256,256,3)))\n\n#print(Photo_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:07.602992Z","iopub.execute_input":"2025-03-04T19:41:07.603403Z","iopub.status.idle":"2025-03-04T19:41:07.608014Z","shell.execute_reply.started":"2025-03-04T19:41:07.603365Z","shell.execute_reply":"2025-03-04T19:41:07.606890Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#plt.imshow(Monet_Tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:07.609197Z","iopub.execute_input":"2025-03-04T19:41:07.609539Z","iopub.status.idle":"2025-03-04T19:41:07.634090Z","shell.execute_reply.started":"2025-03-04T19:41:07.609514Z","shell.execute_reply":"2025-03-04T19:41:07.633108Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#plt.imshow(Photo_train_images[0][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:07.635205Z","iopub.execute_input":"2025-03-04T19:41:07.635530Z","iopub.status.idle":"2025-03-04T19:41:07.657531Z","shell.execute_reply.started":"2025-03-04T19:41:07.635491Z","shell.execute_reply":"2025-03-04T19:41:07.656421Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"I struggled immensely with this project and took a lot of guidance from https://www.kaggle.com/code/sai10py/gans-art-creation to get the dataset into position to load, then saw that cycleGAN is good at training images without a pair. I used most of the example code from https://keras.io/examples/generative/cyclegan/. The images have been split into train and test datasets for both paintings and photos.","metadata":{}},{"cell_type":"code","source":"#Setting Up Dataset to run through GAN, inspired by https://www.kaggle.com/code/victorsullivan/i-m-something-of-a-painter-myself\n\n#kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n#gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n#batch_size=1\n\n#Monet_train_images = (Monet_train_images)\n#Photo_train_images = (Photo_train_images)\n\n#Monet_test_images = (Monet_test_images)\n#Photo_test_images = (Photo_test_images)\n\n#_, ax = plt.subplots(4, 2, figsize=(10, 15))\n#for i, samples in enumerate(zip(Monet_train_images[0][0:4][0:255][0:255], Photo_train_images[0][0:4][0:255][0:255])):\n#    ax[i, 0].imshow(samples[0])\n#    ax[i,1].imshow(samples[1])\n#plt.show()\n#gan_train_dataset = tf.data.Dataset.zip((Monet_train_images, Photo_train_images))\n\n#gan_test_dataset = tf.data.Dataset.zip((Monet_test_images, Photo_test_images))\n#def make_dataset(monet_files, photo_files, batch_size=1):\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:07.658654Z","iopub.execute_input":"2025-03-04T19:41:07.658950Z","iopub.status.idle":"2025-03-04T19:41:07.679635Z","shell.execute_reply.started":"2025-03-04T19:41:07.658925Z","shell.execute_reply":"2025-03-04T19:41:07.678511Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"#_, ax = plt.subplots(4, 2, figsize=(10,15))\n#for i, samples in enumerate(zip(Monet_data[0][0:4][0:255][0:255], Photo_data[0][0:4][0:255][0:255])):\n#    Monet = samples[0]\n#    Photo = samples[1]\n#    ax[i,0].imshow(Monet)\n#    ax[i,1].imshow(Photo)\n\n#plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:07.681021Z","iopub.execute_input":"2025-03-04T19:41:07.681443Z","iopub.status.idle":"2025-03-04T19:41:07.699431Z","shell.execute_reply.started":"2025-03-04T19:41:07.681404Z","shell.execute_reply":"2025-03-04T19:41:07.698237Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"#Discriminator Model interpolated from https://keras.io/examples/generative/dcgan_overriding_train_step/\nmonet_discriminator = tf.keras.Sequential()\nmonet_discriminator.add(tf.keras.layers.InputLayer(shape=(256, 256, 3)))\nmonet_discriminator.add(tf.keras.layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", kernel_initializer=kernel_init))\nmonet_discriminator.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\nmonet_discriminator.add(tf.keras.layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", kernel_initializer=kernel_init))\nmonet_discriminator.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\nmonet_discriminator.add(tf.keras.layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", kernel_initializer=kernel_init))\nmonet_discriminator.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\nmonet_discriminator.add(tf.keras.layers.Flatten())\nmonet_discriminator.add(tf.keras.layers.Dropout(0.2))\nmonet_discriminator.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n\nmonet_discriminator.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:07.702250Z","iopub.execute_input":"2025-03-04T19:41:07.702624Z","iopub.status.idle":"2025-03-04T19:41:07.908537Z","shell.execute_reply.started":"2025-03-04T19:41:07.702580Z","shell.execute_reply":"2025-03-04T19:41:07.907616Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_40 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m12,544\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_14 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_41 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │       \u001b[38;5;34m2,097,664\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_15 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_42 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │       \u001b[38;5;34m4,194,816\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_16 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m524288\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m524288\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │         \u001b[38;5;34m524,289\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,816</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">524288</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">524288</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,289</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,829,313\u001b[0m (26.05 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,829,313</span> (26.05 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,829,313\u001b[0m (26.05 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,829,313</span> (26.05 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"#Generator Model interpolated from https://keras.io/examples/generative/dcgan_overriding_train_step/\nlatent_dim = 512\n\nmonet_generator=tf.keras.Sequential()\nmonet_generator.add(tf.keras.layers.InputLayer(shape=(32, 256, 3)))\n#monet_generator.add(keras.layers.Dense(32*32*512))\n#monet_generator.add(keras.layers.Reshape((32, 32, 512)))\nmonet_generator.add(tf.keras.layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", kernel_initializer=kernel_init))\nmonet_generator.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\nmonet_generator.add(tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", kernel_initializer=kernel_init))\nmonet_generator.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\nmonet_generator.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\", kernel_initializer=kernel_init))\nmonet_generator.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\nmonet_generator.add(tf.keras.layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\", kernel_initializer=kernel_init))\n\nmonet_generator.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:07.909738Z","iopub.execute_input":"2025-03-04T19:41:07.909999Z","iopub.status.idle":"2025-03-04T19:41:08.003024Z","shell.execute_reply.started":"2025-03-04T19:41:07.909979Z","shell.execute_reply":"2025-03-04T19:41:08.001080Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_transpose_7 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m3,136\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_17 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose_8 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m131,200\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_18 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose_9 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m2048\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m524,544\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_19 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m2048\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_43 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m2048\u001b[0m, \u001b[38;5;34m3\u001b[0m)        │          \u001b[38;5;34m19,203\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_transpose_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">19,203</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m678,083\u001b[0m (2.59 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">678,083</span> (2.59 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m678,083\u001b[0m (2.59 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">678,083</span> (2.59 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"#Inspired from https://keras.io/examples/generative/cyclegan/\nclass Reflection_Padding2D(layers.Layer):\n    def __init__(self, padding=(1,1), **kwargs):\n        self.padding = tuple(padding)\n        super().__init__(**kwargs)\n\n    def call(self, input_tensor, mask=None):\n        padding_width, padding_height = self.padding\n        padding_tensor = [ [0,0], [padding_height, padding_height], [padding_width, padding_width], [0,0]]\n        return ops.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n\ndef residual_block(x, activation, kernel_initializer=kernel_init, kernel_size=(3,3), strides=(1,1), padding=\"valid\", gamma_initializer = gamma_init, use_bias=False):\n    dim = x.shape[-1]\n    input_tensor = x\n\n    x = Reflection_Padding2D()(input_tensor)\n    x = layers.Conv2D(dim, kernel_size, strides=strides, kernel_initializer=kernel_initializer, padding=padding, use_bias = use_bias,)(x)\n    x = tf.keras.layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n    x = layers.add([input_tensor, x])\n    return x\n\ndef downsample(x, filters, activation, kernel_initializer=kernel_init, kernel_size=(3,3), strides=(2,2),padding=\"same\",gamma_initializer=gamma_init, use_bias=False):\n    x = layers.Conv2D(filters, kernel_size, strides=strides, kernel_initializer=kernel_initializer, padding=padding, use_bias = use_bias,)(x)\n    x = tf.keras.layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n    if activation:\n        x = activation(x)\n    return x\n\ndef upsample(x, filters, activation, kernel_size=(3,3),strides=(2,2), padding=\"same\", kernel_initializer=kernel_init, gamma_initializer=gamma_init, use_bias=False):\n    x = layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding, kernel_initializer=kernel_initializer, use_bias=use_bias,)(x)\n    x = tf.keras.layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n    if activation:\n        x = activation(x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:08.003791Z","iopub.execute_input":"2025-03-04T19:41:08.004085Z","iopub.status.idle":"2025-03-04T19:41:08.015300Z","shell.execute_reply.started":"2025-03-04T19:41:08.004062Z","shell.execute_reply":"2025-03-04T19:41:08.014076Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"#Inspired from https://keras.io/examples/generative/cyclegan/\ndef get_resnet_generator(filters=64, num_downsampling_blocks=2, num_residual_blocks=9, num_upsample_blocks=2, gamma_initializer=gamma_init, name=None):\n    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n    x = Reflection_Padding2D(padding=(3,3))(img_input)\n    x = layers.Conv2D(filters, (7,7), kernel_initializer=kernel_init, use_bias=False)(x)\n    x = tf.keras.layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n    x = layers.Activation(\"relu\")(x)\n\n    #Downsampling\n    for _ in range(num_downsampling_blocks):\n        filters *= 2\n        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n\n    #Residual Blocks\n    for _ in range(num_residual_blocks):\n        x = residual_block(x, activation=layers.Activation(\"relu\"))\n\n    #Upsampling\n    for _ in range(num_upsample_blocks):\n        filters //=2\n        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n\n    #Final block\n    x = Reflection_Padding2D(padding=(3,3))(x)\n    x = layers.Conv2D(3, (7,7), padding=\"valid\")(x)\n    x = layers.Activation(\"tanh\")(x)\n\n    model = tf.keras.models.Model(img_input, x, name=name)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:08.016704Z","iopub.execute_input":"2025-03-04T19:41:08.017086Z","iopub.status.idle":"2025-03-04T19:41:08.041371Z","shell.execute_reply.started":"2025-03-04T19:41:08.017060Z","shell.execute_reply":"2025-03-04T19:41:08.040157Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def get_discriminator(filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None):\n    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n    x = layers.Conv2D(filters,(4,4),strides=(2,2),padding=\"same\",kernel_initializer=kernel_initializer,)(img_input)\n    x = layers.LeakyReLU(0.2)(x)\n\n    num_filters=filters\n    for num_downsample_block in range(3):\n        num_filters *=2\n        if num_downsample_block<2:\n            x = downsample(x, filters=num_filters, activation=layers.LeakyReLU(0.2),kernel_size=(4,4),strides=(2,2),)\n        else:\n            x = downsample(x, filters=num_filters, activation=layers.LeakyReLU(0.2),kernel_size=(4,4),strides=(1,1),)\n\n    x = layers.Conv2D(1, (4,4), strides=(1,1), padding=\"same\", kernel_initializer=kernel_initializer)(x)\n    model = tf.keras.models.Model(inputs=img_input, outputs=x, name=name)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:08.042687Z","iopub.execute_input":"2025-03-04T19:41:08.043022Z","iopub.status.idle":"2025-03-04T19:41:08.069429Z","shell.execute_reply.started":"2025-03-04T19:41:08.042994Z","shell.execute_reply":"2025-03-04T19:41:08.068241Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"#Get generators and discriminators\ngen_F = get_resnet_generator(name=\"generator_F\")\ngen_G = get_resnet_generator(name=\"generator_G\")\n\ndisc_X = get_discriminator(name=\"discriminator_X\")\ndisc_Y = get_discriminator(name=\"discriminator_Y\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:41:08.070563Z","iopub.execute_input":"2025-03-04T19:41:08.071019Z","iopub.status.idle":"2025-03-04T19:41:08.916777Z","shell.execute_reply.started":"2025-03-04T19:41:08.070976Z","shell.execute_reply":"2025-03-04T19:41:08.915606Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"#GAN model to override train_step interpolated from https://keras.io/examples/generative/dcgan_overriding_train_step/\n#%%Need to work on removing random and replacing with photo%%\nclass monet_GAN(tf.keras.Model):\n    def __init__(self, generator_G, generator_F, discriminator_X, discriminator_Y, lambda_cycle=10.0, lambda_identity=0.5):\n        super().__init__()\n        self.gen_G = generator_G\n        self.gen_F = generator_F\n        self.disc_X = discriminator_X\n        self.disc_Y = discriminator_Y\n        self.lambda_cycle = lambda_cycle\n        self.lambda_identity = lambda_identity\n        #self.seed_generator = keras.random.SeedGenerator(1996)\n\n    def call(self, inputs):\n        return(self.disc_X(inputs), self.disc_Y(inputs),self.gen_G(inputs), self.gen_F(inputs))\n    \n    def compile(self, gen_G_opt, gen_F_opt, disc_X_opt, disc_Y_opt, gen_loss_func, disc_loss_func):\n        super().compile()\n        self.gen_G_opt = gen_G_opt\n        self.gen_F_opt = gen_F_opt\n        self.disc_X_opt = disc_X_opt\n        self.disc_Y_opt = disc_Y_opt\n        self.gen_loss_func = gen_loss_func\n        self.disc_loss_func = disc_loss_func\n        self.cycle_loss_func = tf.keras.losses.MeanAbsoluteError()\n        self.identity_loss_func = tf.keras.losses.MeanAbsoluteError()\n\n    #@property\n    #def metrics(self):\n    #    return [self.d_loss_metric, self.g_loss_metric]\n\n    def train_step(self, batch_data):\n        # Sample random points in the latent space\n        #batch_size = ops.shape(images[0])[0]\n        #random_latent_vectors = keras.random.normal(\n        #    shape=(batch_size, self.latent_dim), seed=self.seed_generator\n        #)\n        #print(batch_data.size)\n        #monet_img = batch_data[:][0:225][0:255][0:255][0:2]\n        #photo_img = batch_data[:][0:5278][0:255][0:255][0:2]\n        #batch_size = tf.shape(real_monet)[0]\n        # Decode them to photo images\n        real_x, real_y = batch_data\n        #generated_images = self.generator(photo_img)\n\n        # Combine them with real images\n        #combined_images = ops.concatenate([monet_img, photo_img], axis=0)\n\n        # Assemble labels discriminating real from fake images\n        #labels = ops.concatenate(\n        #    [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n        #)\n        # Add random noise to the labels - important trick!\n        #labels += 0.05 * tf.random.uniform(tf.shape(labels))\n\n        # Train the discriminator\n        with tf.GradientTape(persistent=True) as tape:\n            fake_y = self.gen_G(real_x, training=True)\n            fake_x = self.gen_F(real_y, training=True)\n            cycled_x = self.gen_F(fake_y, training=True)\n            cycled_y = self.gen_G(fake_x, training=True)\n\n            #Identity mapping\n            same_x = self.gen_F(real_x, training=True)\n            same_y = self.gen_G(real_y, training=True)\n\n            disc_real_x = self.disc_X(real_x, training=True)\n            disc_fake_x = self.disc_X(fake_x, training=True)\n\n            disc_real_y = self.disc_Y(real_y, training=True)\n            disc_fake_y = self.disc_Y(fake_y, training=True)\n\n            #Gen adv loss\n            gen_G_loss = self.gen_loss_func(disc_fake_y)\n            gen_F_loss = self.gen_loss_func(disc_fake_x)\n\n            #Gen cycle loss\n            cycle_loss_G = self.cycle_loss_func(real_y, cycled_y)*self.lambda_cycle\n            cycle_loss_F = self.cycle_loss_func(real_x, cycled_x)*self.lambda_cycle\n\n            #Gen Identity loss\n            id_loss_G = (self.identity_loss_func(real_y, same_y)*self.lambda_cycle*self.lambda_identity)\n            id_loss_F = (self.identity_loss_func(real_x, same_x)*self.lambda_cycle*self.lambda_identity)\n\n            #Total Gen loss\n            total_loss_G = gen_G_loss+cycle_loss_G+id_loss_G\n            total_loss_F = gen_F_loss+cycle_loss_F+id_loss_F\n\n            #Total Discriminator loss\n            disc_X_loss = self.disc_loss_func(disc_real_x, disc_fake_x)\n            disc_Y_loss = self.disc_loss_func(disc_real_y, disc_fake_y)\n\n        #Get the gradients for generators\n        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n\n        #Get gradients for discriminators\n        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n\n        #++JG\n        #print(grads_G)\n        #print(self.grads_G.trainable_variables)\n        #Tried removing self from grads_G\n        #--JG\n        #Update the weights of the generators\n        self.gen_G_opt.apply_gradients(zip(grads_G, self.gen_G.trainable_variables))\n        self.gen_F_opt.apply_gradients(zip(grads_F, self.gen_F.trainable_variables))\n\n        #Update weights of discriminators\n        self.disc_X_opt.apply_gradients(zip(disc_X_grads, self.disc_X.trainable_variables))\n        self.disc_Y_opt.apply_gradients(zip(disc_Y_grads, self.disc_Y.trainable_variables))\n\n        return {\n            \"gen_1_loss\": total_loss_G,\n            \"gen_2_loss\": total_loss_F,\n            \"disc_1_loss\": disc_X_loss,\n            \"disc_2_loss\": disc_Y_loss,\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:48:45.185437Z","iopub.execute_input":"2025-03-04T19:48:45.185940Z","iopub.status.idle":"2025-03-04T19:48:45.202153Z","shell.execute_reply.started":"2025-03-04T19:48:45.185904Z","shell.execute_reply":"2025-03-04T19:48:45.200389Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"#Training the model\nadv_loss_func = tf.keras.losses.MeanSquaredError()\n\ndef gen_loss_func(fake):\n    fake_loss = adv_loss_func(ops.ones_like(fake),fake)\n    return fake_loss\n\ndef disc_loss_func(real, fake):\n    real_loss = adv_loss_func(ops.ones_like(real),real)\n    fake_loss = adv_loss_func(ops.zeros_like(fake),fake)\n    return (real_loss + fake_loss) * 0.5\n\nMonet_gan_model = monet_GAN(generator_F = gen_F, generator_G = gen_G, discriminator_X = disc_X, discriminator_Y = disc_Y)\n\nMonet_gan_model.compile(\n    gen_F_opt=tf.keras.optimizers.SGD(learning_rate=2e-4),\n    gen_G_opt=tf.keras.optimizers.SGD(learning_rate=2e-4),\n    disc_X_opt=tf.keras.optimizers.SGD(learning_rate=2e-4),\n    disc_Y_opt=tf.keras.optimizers.SGD(learning_rate=2e-4),\n    gen_loss_func = gen_loss_func,\n    disc_loss_func = disc_loss_func,\n)\n\n#Callbacks\nepochs = 1\nMonet_gan_model.fit(\n    tf.data.Dataset.zip((monet_data, photo_data)),\n    epochs=1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:48:48.498240Z","iopub.execute_input":"2025-03-04T19:48:48.498801Z","iopub.status.idle":"2025-03-04T21:22:24.024568Z","shell.execute_reply.started":"2025-03-04T19:48:48.498764Z","shell.execute_reply":"2025-03-04T21:22:24.022365Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5615s\u001b[0m 19s/step - disc_1_loss: 0.4294 - disc_2_loss: 0.4291 - gen_1_loss: 8.0279 - gen_2_loss: 6.7344\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7f5f03caaa70>"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"Due to my struggles I ran out of time with this project and need to submit this model. I would approach changing the number of epochs and changing the optimizers and their learning rate.","metadata":{}},{"cell_type":"code","source":"! mkdir ../images\n    \ni = 1\nfor img in photo_data:\n    prediction = Monet_gan_model.gen_G(img, training=False)[0].numpy()\n    prediction = (prediction*127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images\" + str(i) + \".jpg\")\n\nshutil.make_archive(\"/kaggle/working/images\",\"zip\",\"/kaggle/images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T22:18:05.412557Z","iopub.execute_input":"2025-03-04T22:18:05.413057Z"}},"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘../images’: File exists\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"My submission got a score of %%. I learned that coding all night is a good way to make careless mistakes and that the TA for this course is extremely helpful","metadata":{}}]}